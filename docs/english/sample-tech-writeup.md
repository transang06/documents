# Sample Technical Writeup â€“ RAG with LangChain
 <!-- BÃ i máº«u viáº¿t ká»¹ thuáº­t ngáº¯n báº±ng tiáº¿ng Anh/ -->
## ğŸ” Problem
We wanted to build a question-answering system that can access up-to-date internal documentation instead of relying only on pre-trained LLM knowledge.

## ğŸ’¡ Solution
We implemented a Retrieval-Augmented Generation (RAG) pipeline using LangChain. ChromaDB was used to store vectorized documents, and Groq LLM served as the generator.

## âš™ï¸ Architecture

1. Documents are chunked and embedded using `intfloat/multilingual-e5-base`.
2. ChromaDB stores the embeddings.
3. At query time, the retriever fetches relevant chunks.
4. The generator produces the answer with context-aware information.

## ğŸ§ª Result
The system achieved more accurate and explainable answers compared to using GPT-4 alone.

## ğŸ“Œ Tech Stack
- LangChain
- ChromaDB
- Groq LLM
- FastAPI
- SentenceTransformer

## âœï¸ Sample sentence
> â€œWe used a RAG pipeline to enable real-time domain-specific question answering in Vietnamese.â€
